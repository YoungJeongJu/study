{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TabNet\n",
    "-----\n",
    "\n",
    "* self-supervised 관련 리서치 중 대부분 이미지 데이터, 음성 데이터 등 비정형 데이터에 치중되어있다. \n",
    "* tabular data나 정형데이터에 적합한 self-supervised learning 방법이 없을까? 라는 생각에서 research를 해보았다.\n",
    "\n",
    "* Arik S, & Pfister T. *TabNet: Attentive Interpretable Tabular Learning*\n",
    "* https://arxiv.org/abs/1908.07442"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-11T02:03:53.970936Z",
     "start_time": "2022-02-11T02:03:52.572030Z"
    }
   },
   "outputs": [],
   "source": [
    "## Package Setting\n",
    "\n",
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import random as rn\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "from numpy.random import rand, randn, randint\n",
    "\n",
    "from scipy.special import softmax\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style='whitegrid', context='notebook')\n",
    "# %matplotlib notebook\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score, average_precision_score, roc_auc_score, roc_curve, confusion_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-11T02:03:57.853067Z",
     "start_time": "2022-02-11T02:03:54.327101Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.metrics import Mean\n",
    "\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow_addons.activations import sparsemax\n",
    "\n",
    "\n",
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "disable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 주요특징\n",
    "\n",
    "* 좌에서 우로 가면서 (sequential)하게 학습하며 feature selection을 하면서 학습하는 형식이다.\n",
    "* Supervised fine tuning 하는 방법과 Unsupervised pre-trainining 방식이 있다. 왼쪽에 설명된 방식이 self-supervised learning과 관계가 있는데Encoder-Decoder 구조의 비지도 학습을 수행하여 masked 된 변수값을 예측한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-11T01:15:16.131255Z",
     "start_time": "2022-02-11T01:15:15.978396Z"
    }
   },
   "source": [
    "<!-- ![tabnet_sparse_feature_selection](./tabnet_sparse_feature_selection.png) -->\n",
    "<!-- <figure>\n",
    "<img src=\"/Users/youngjeongju/Desktop/project/study/autoencoder/tabnet_sparse_feature_selection.png\" alt=\"feature selection\" style=\"width: 1000px;\"/>\n",
    "</figure> -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tabnet_sparse_feature_selection](./tabnet_sparse_feature_selection.png)\n",
    "![Alt text](./tabnet_training_tuning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-10T12:26:59.080963Z",
     "start_time": "2022-02-10T12:26:58.961953Z"
    }
   },
   "source": [
    "![Alt text](./tabnet_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Encoder\n",
    "\n",
    "* 각 decision step 내에서 feature transformer, attentive transformer, feature masking으로 구성되어있다. \n",
    "* split block은 feature transformer로부터 나온 representation을 두개로 나누어 하나는 ReLU를 태워 최종 output으로, 다른 하나는 attentive transformer로 넘겨준다.\n",
    "* Mask block은 각 step에서 feature가 어떻게 작동하는지 알 수 있고 집계하여 feature importance를 확인할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](./tabnet_ft.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Transformer\n",
    "\n",
    "* feature block 들이 연속적으로 이어져 있는 것으로 각  block은 Fully-Connected Layer(FC)와 Batch Normalization(BN), GLU(Gated Linear Unit) 로 구성되어있다.\n",
    "* 한 block의 과정이 이루어진 후에 안정성 확보를 위해 sqrt(0.5)를 곱한다. \n",
    "* 예시) 2 레이어는 모든 decision 단계에서 공유되고, 나머지 2 레이어는 해당 decision step에서만 사용한다.\n",
    "* 최종적으로 2개의 output이 출력된다. \n",
    "    - output 예측\n",
    "    - 다음 step의 attentive transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GLU (Gated Linear Unit)\n",
    "* 이전 레이어에서 나오는 정보를 제어하는 역할로 gradient가 급격하게 작아지거나 커지는 것을 막는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-10T17:03:05.240170Z",
     "start_time": "2022-02-10T17:03:05.235836Z"
    }
   },
   "outputs": [],
   "source": [
    "def glu(x, n_units=None):\n",
    "    \"\"\"Generalized linear unit nonlinear activation.\"\"\"\n",
    "    if n_units is None:\n",
    "        n_units = tf.shape(x)[-1] // 2\n",
    "\n",
    "    return x[..., :n_units] * tf.nn.sigmoid(x[..., n_units:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-10T17:03:05.248507Z",
     "start_time": "2022-02-10T17:03:05.242411Z"
    }
   },
   "outputs": [],
   "source": [
    "class FeatureBlock(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Implementation of a FL->BN->GLU block\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_dim,\n",
    "        apply_glu = True,\n",
    "        bn_momentum = 0.9,\n",
    "        fc = None,\n",
    "        epsilon = 1e-5,\n",
    "    ):\n",
    "        super(FeatureBlock, self).__init__()\n",
    "        self.apply_gpu = apply_glu\n",
    "        self.feature_dim = feature_dim\n",
    "        units = feature_dim * 2 if apply_glu else feature_dim # desired dimension gets multiplied by 2\n",
    "                                                              # because GLU activation halves it\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(units, use_bias=False) if fc is None else fc # shared layers can get re-used\n",
    "        self.bn = tf.keras.layers.BatchNormalization(momentum=bn_momentum, epsilon=epsilon)\n",
    "\n",
    "    def call(self, x, training = None):\n",
    "        x = self.fc(x) # inputs passes through the FC layer\n",
    "        x = self.bn(x, training=training) # FC layer output gets passed through the BN\n",
    "        if self.apply_gpu: \n",
    "            return glu(x, self.feature_dim) # GLU activation applied to BN output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-10T17:03:05.259311Z",
     "start_time": "2022-02-10T17:03:05.251612Z"
    }
   },
   "outputs": [],
   "source": [
    "class FeatureTransformer(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_dim,\n",
    "        fcs = [],\n",
    "        n_total = 4,\n",
    "        n_shared = 2,\n",
    "        bn_momentum = 0.9,\n",
    "    ):\n",
    "        super(FeatureTransformer, self).__init__()\n",
    "        self.n_total, self.n_shared = n_total, n_shared\n",
    "\n",
    "        kwrgs = {\n",
    "            \"feature_dim\": feature_dim,\n",
    "            \"bn_momentum\": bn_momentum,\n",
    "        }\n",
    "\n",
    "        # build blocks\n",
    "        self.blocks = []\n",
    "        for n in range(n_total):\n",
    "            # some shared blocks\n",
    "            if fcs and n < len(fcs):\n",
    "                self.blocks.append(FeatureBlock(**kwrgs, fc=fcs[n])) # Building shared blocks by providing FC layers\n",
    "            # build new blocks\n",
    "            else:\n",
    "                self.blocks.append(FeatureBlock(**kwrgs)) # Step dependent blocks without the shared FC layers\n",
    "\n",
    "    def call(self, x, training = None):\n",
    "        # input passes through the first block\n",
    "        x = self.blocks[0](x, training=training) \n",
    "        # for the remaining blocks\n",
    "        for n in range(1, self.n_total):\n",
    "            # output from previous block gets multiplied by sqrt(0.5) and output of this block gets added\n",
    "            x = x * tf.sqrt(0.5) + self.blocks[n](x, training=training) \n",
    "        return x\n",
    "\n",
    "    @property\n",
    "    def shared_fcs(self):\n",
    "        return [self.blocks[i].fc for i in range(self.n_shared)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Attentive Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](./tabnet_at.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Attentive Transformer는 feature transformer에서 split되어 나온 output중 하나를 input으로 받아 FC, BN, Prior scales, Sparsemax layer를 통과한다. \n",
    "* prior scale을 고려한 sparsemax 활성화를 통해 각 단계에서 feature을 selection한다. \n",
    "* 인코딩된 decision은 attentive transformer 블록을 거쳐 trainable mask로 변환된다.\n",
    "* 예시) 단일 네트워크 구조의 attentive transformer 구조"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sparsemax activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* sparse한 데이터셋에 적용했을 때 좋은 성능을 보이는 activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-10T17:03:05.267827Z",
     "start_time": "2022-02-10T17:03:05.263661Z"
    }
   },
   "outputs": [],
   "source": [
    "class AttentiveTransformer(tf.keras.Model):\n",
    "    def __init__(self, feature_dim):\n",
    "        super(AttentiveTransformer, self).__init__()\n",
    "        self.block = FeatureBlock(\n",
    "            feature_dim,\n",
    "            apply_glu=False, #sparsemax instead of glu\n",
    "        )\n",
    "\n",
    "    def call(self, x, prior_scales, training=None):\n",
    "        x = self.block(x, training=training)\n",
    "        return sparsemax(x * prior_scales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature selection 하는 과정\n",
    "\n",
    "* prior scale layer는 이전 step까지 각 기능이 얼마나 많이 사용되었는지 합산한다. \n",
    "* mask : 어떤 feature을 주로 사용할 것인지에 대한 정보가 있음. 다음 step에서 mask 정보를 재활용할 수 있다.\n",
    "* mask를 얼마나 재활용할지를 relaxation factor($\\gamma$)을 통해 조절한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](./tabnet_attentive_transformer_ex.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 선택된 feature들의 sparsity를 컨트롤하기 위해서 entropy form 안에서 sparsity 정규화를 실행한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](./tabnet_sparsity.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](./tabnet_decoder.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 각 단계에서 feature transformer 블록으로 구성된다.\n",
    "* Self-supervised learning : Decoder의 부분을 통해서 pre-training이 이루어진다.\n",
    "    - tabnet encoder를 통해서 feature engineering 효과를 내고, decision making 부분을 통해 feature selection을 한다.\n",
    "    - 서로 관계가 있는 변수들은 마스킹된 자체 모니터링 학습을 통해서 feature이 재구성되며 학습 작업의 인코더 모델을 개선한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TabNet full code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-10T17:03:05.285518Z",
     "start_time": "2022-02-10T17:03:05.271378Z"
    }
   },
   "outputs": [],
   "source": [
    "# TabNet\n",
    "\n",
    "class TabNet(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_features,\n",
    "        feature_dim,\n",
    "        output_dim,\n",
    "        n_step = 2,\n",
    "        n_total = 4,\n",
    "        n_shared = 2,\n",
    "        relaxation_factor = 1.5,\n",
    "        bn_epsilon = 1e-5,\n",
    "        bn_momentum = 0.7,\n",
    "        sparsity_coefficient = 1e-5\n",
    "    ):\n",
    "        super(TabNet, self).__init__()\n",
    "        self.output_dim, self.num_features = output_dim, num_features\n",
    "        self.n_step, self.relaxation_factor = n_step, relaxation_factor\n",
    "        self.sparsity_coefficient = sparsity_coefficient\n",
    "\n",
    "        self.bn = tf.keras.layers.BatchNormalization(\n",
    "            momentum=bn_momentum, epsilon=bn_epsilon\n",
    "        )\n",
    "\n",
    "        kargs = {\n",
    "            \"feature_dim\": feature_dim + output_dim,\n",
    "            \"n_total\": n_total,\n",
    "            \"n_shared\": n_shared,\n",
    "            \"bn_momentum\": bn_momentum\n",
    "        }\n",
    "\n",
    "        # first feature transformer block is built first to get the shared blocks\n",
    "        self.feature_transforms = [FeatureTransformer(**kargs)]\n",
    "        self.attentive_transforms = []\n",
    "            \n",
    "        # each step consists out of FT and AT\n",
    "        for i in range(n_step):\n",
    "            self.feature_transforms.append(\n",
    "                FeatureTransformer(**kargs, fcs=self.feature_transforms[0].shared_fcs)\n",
    "            )\n",
    "            self.attentive_transforms.append(\n",
    "                AttentiveTransformer(num_features)\n",
    "            )\n",
    "        \n",
    "        # Final output layer\n",
    "        self.head = tf.keras.layers.Dense(2, activation=\"softmax\", use_bias=False)\n",
    "\n",
    "    def call(self, features, training = None):\n",
    "\n",
    "        bs = tf.shape(features)[0] # get batch shape\n",
    "        out_agg = tf.zeros((bs, self.output_dim)) # empty array with outputs to fill\n",
    "        prior_scales = tf.ones((bs, self.num_features)) # prior scales initialised as 1s\n",
    "        importance = tf.zeros([bs, self.num_features]) # importances\n",
    "        masks = []\n",
    "\n",
    "        features = self.bn(features, training=training) # Batch Normalisation\n",
    "        masked_features = features\n",
    "\n",
    "        total_entropy = 0.0\n",
    "\n",
    "        for step_i in range(self.n_step + 1):\n",
    "            # (masked) features go through the FT\n",
    "            x = self.feature_transforms[step_i](\n",
    "                masked_features, training=training\n",
    "            )\n",
    "            \n",
    "            # first FT is not used to generate output\n",
    "            if step_i > 0:\n",
    "                # first half of the FT output goes towards the decision \n",
    "                out = tf.keras.activations.relu(x[:, : self.output_dim])\n",
    "                out_agg += out\n",
    "                scale_agg = tf.reduce_sum(out, axis=1, keepdims=True) / (self.n_step - 1)\n",
    "                importance += mask_values * scale_agg\n",
    "                \n",
    "\n",
    "            # no need to build the features mask for the last step\n",
    "            if step_i < self.n_step:\n",
    "                # second half of the FT output goes as input to the AT\n",
    "                x_for_mask = x[:, self.output_dim :]\n",
    "                \n",
    "                # apply AT with prior scales\n",
    "                mask_values = self.attentive_transforms[step_i](\n",
    "                    x_for_mask, prior_scales, training=training\n",
    "                )\n",
    "\n",
    "                # recalculate the prior scales\n",
    "                prior_scales *= self.relaxation_factor - mask_values\n",
    "                \n",
    "                # multiply the second half of the FT output by the attention mask to enforce sparsity\n",
    "                masked_features = tf.multiply(mask_values, features)\n",
    "\n",
    "                # entropy is used to penalize the amount of sparsity in feature selection\n",
    "                total_entropy += tf.reduce_mean(\n",
    "                    tf.reduce_sum(\n",
    "                        tf.multiply(-mask_values, tf.math.log(mask_values + 1e-15)),\n",
    "                        axis=1,\n",
    "                    )\n",
    "                )\n",
    "                \n",
    "                # append mask values for later explainability\n",
    "                masks.append(tf.expand_dims(tf.expand_dims(mask_values, 0), 3))\n",
    "                \n",
    "        #Per step selection masks        \n",
    "        self.selection_masks = masks\n",
    "        \n",
    "        # Final output\n",
    "        final_output = self.head(out)\n",
    "        \n",
    "        # Add sparsity loss\n",
    "        loss = total_entropy / (self.n_step-1)\n",
    "        self.add_loss(self.sparsity_coefficient * loss)\n",
    "        \n",
    "        return final_output, importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Fraud Detection \n",
    "\n",
    "- Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-10T16:44:32.509641Z",
     "start_time": "2022-02-10T16:44:32.505839Z"
    }
   },
   "source": [
    "### Data\n",
    "* Data Shape : (284807, 31)\n",
    "* x: v1~v28, time, amount\n",
    "* y: label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-10T17:03:07.378944Z",
     "start_time": "2022-02-10T17:03:05.287452Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(284807, 31)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ = pd.read_csv('./data/creditcard.csv').reset_index(drop=True)\n",
    "\n",
    "# 데이터 변수 조정\n",
    "\n",
    "df_.columns = map(str.lower, df_.columns)\n",
    "df_.rename(columns={'class': 'label'}, inplace=True)\n",
    "\n",
    "df_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "* amount 변수 log 형태로 전환\n",
    "* time 변수 제거\n",
    "* minmax scaler 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-10T17:03:08.947707Z",
     "start_time": "2022-02-10T17:03:07.381693Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>v3</th>\n",
       "      <th>v4</th>\n",
       "      <th>v5</th>\n",
       "      <th>v6</th>\n",
       "      <th>v7</th>\n",
       "      <th>v8</th>\n",
       "      <th>v9</th>\n",
       "      <th>v10</th>\n",
       "      <th>...</th>\n",
       "      <th>v21</th>\n",
       "      <th>v22</th>\n",
       "      <th>v23</th>\n",
       "      <th>v24</th>\n",
       "      <th>v25</th>\n",
       "      <th>v26</th>\n",
       "      <th>v27</th>\n",
       "      <th>v28</th>\n",
       "      <th>amount</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.935192</td>\n",
       "      <td>0.766490</td>\n",
       "      <td>0.881365</td>\n",
       "      <td>0.313023</td>\n",
       "      <td>0.763439</td>\n",
       "      <td>0.267669</td>\n",
       "      <td>0.266815</td>\n",
       "      <td>0.786444</td>\n",
       "      <td>0.475312</td>\n",
       "      <td>0.510600</td>\n",
       "      <td>...</td>\n",
       "      <td>0.561184</td>\n",
       "      <td>0.522992</td>\n",
       "      <td>0.663793</td>\n",
       "      <td>0.391253</td>\n",
       "      <td>0.585122</td>\n",
       "      <td>0.394557</td>\n",
       "      <td>0.418976</td>\n",
       "      <td>0.312697</td>\n",
       "      <td>0.493873</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.978542</td>\n",
       "      <td>0.770067</td>\n",
       "      <td>0.840298</td>\n",
       "      <td>0.271796</td>\n",
       "      <td>0.766120</td>\n",
       "      <td>0.262192</td>\n",
       "      <td>0.264875</td>\n",
       "      <td>0.786298</td>\n",
       "      <td>0.453981</td>\n",
       "      <td>0.505267</td>\n",
       "      <td>...</td>\n",
       "      <td>0.557840</td>\n",
       "      <td>0.480237</td>\n",
       "      <td>0.666938</td>\n",
       "      <td>0.336440</td>\n",
       "      <td>0.587290</td>\n",
       "      <td>0.446013</td>\n",
       "      <td>0.416345</td>\n",
       "      <td>0.313423</td>\n",
       "      <td>0.128583</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.935217</td>\n",
       "      <td>0.753118</td>\n",
       "      <td>0.868141</td>\n",
       "      <td>0.268766</td>\n",
       "      <td>0.762329</td>\n",
       "      <td>0.281122</td>\n",
       "      <td>0.270177</td>\n",
       "      <td>0.788042</td>\n",
       "      <td>0.410603</td>\n",
       "      <td>0.513018</td>\n",
       "      <td>...</td>\n",
       "      <td>0.565477</td>\n",
       "      <td>0.546030</td>\n",
       "      <td>0.678939</td>\n",
       "      <td>0.289354</td>\n",
       "      <td>0.559515</td>\n",
       "      <td>0.402727</td>\n",
       "      <td>0.415489</td>\n",
       "      <td>0.311911</td>\n",
       "      <td>0.584923</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.941878</td>\n",
       "      <td>0.765304</td>\n",
       "      <td>0.868484</td>\n",
       "      <td>0.213661</td>\n",
       "      <td>0.765647</td>\n",
       "      <td>0.275559</td>\n",
       "      <td>0.266803</td>\n",
       "      <td>0.789434</td>\n",
       "      <td>0.414999</td>\n",
       "      <td>0.507585</td>\n",
       "      <td>...</td>\n",
       "      <td>0.559734</td>\n",
       "      <td>0.510277</td>\n",
       "      <td>0.662607</td>\n",
       "      <td>0.223826</td>\n",
       "      <td>0.614245</td>\n",
       "      <td>0.389197</td>\n",
       "      <td>0.417669</td>\n",
       "      <td>0.314371</td>\n",
       "      <td>0.475117</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.938617</td>\n",
       "      <td>0.776520</td>\n",
       "      <td>0.864251</td>\n",
       "      <td>0.269796</td>\n",
       "      <td>0.762975</td>\n",
       "      <td>0.263984</td>\n",
       "      <td>0.268968</td>\n",
       "      <td>0.782484</td>\n",
       "      <td>0.490950</td>\n",
       "      <td>0.524303</td>\n",
       "      <td>...</td>\n",
       "      <td>0.561327</td>\n",
       "      <td>0.547271</td>\n",
       "      <td>0.663392</td>\n",
       "      <td>0.401270</td>\n",
       "      <td>0.566343</td>\n",
       "      <td>0.507497</td>\n",
       "      <td>0.420561</td>\n",
       "      <td>0.317490</td>\n",
       "      <td>0.419792</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         v1        v2        v3        v4        v5        v6        v7  \\\n",
       "0  0.935192  0.766490  0.881365  0.313023  0.763439  0.267669  0.266815   \n",
       "1  0.978542  0.770067  0.840298  0.271796  0.766120  0.262192  0.264875   \n",
       "2  0.935217  0.753118  0.868141  0.268766  0.762329  0.281122  0.270177   \n",
       "3  0.941878  0.765304  0.868484  0.213661  0.765647  0.275559  0.266803   \n",
       "4  0.938617  0.776520  0.864251  0.269796  0.762975  0.263984  0.268968   \n",
       "\n",
       "         v8        v9       v10  ...       v21       v22       v23       v24  \\\n",
       "0  0.786444  0.475312  0.510600  ...  0.561184  0.522992  0.663793  0.391253   \n",
       "1  0.786298  0.453981  0.505267  ...  0.557840  0.480237  0.666938  0.336440   \n",
       "2  0.788042  0.410603  0.513018  ...  0.565477  0.546030  0.678939  0.289354   \n",
       "3  0.789434  0.414999  0.507585  ...  0.559734  0.510277  0.662607  0.223826   \n",
       "4  0.782484  0.490950  0.524303  ...  0.561327  0.547271  0.663392  0.401270   \n",
       "\n",
       "        v25       v26       v27       v28    amount  label  \n",
       "0  0.585122  0.394557  0.418976  0.312697  0.493873    0.0  \n",
       "1  0.587290  0.446013  0.416345  0.313423  0.128583    0.0  \n",
       "2  0.559515  0.402727  0.415489  0.311911  0.584923    0.0  \n",
       "3  0.614245  0.389197  0.417669  0.314371  0.475117    0.0  \n",
       "4  0.566343  0.507497  0.420561  0.317490  0.419792    0.0  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# amount\n",
    "df_['amount'] = np.log(df_.amount + 1)\n",
    "\n",
    "# time\n",
    "df_[\"time\"] = df_[\"time\"].apply(lambda x : x / 3600 % 24)\n",
    "\n",
    "# drop time variable\n",
    "df_ = df_.drop('time', axis=1)\n",
    "\n",
    "# scaler\n",
    "scaler = MinMaxScaler()\n",
    "colnames_all = list(df_)\n",
    "df_[colnames_all] = scaler.fit_transform(df_[colnames_all])\n",
    "\n",
    "df_.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data split\n",
    "\n",
    "* df_train, df_valid, df_test  = 6:2:2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-10T17:03:09.073980Z",
     "start_time": "2022-02-10T17:03:08.949200Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train_, df_test = train_test_split(df_, test_size=0.2)\n",
    "df_train, df_valid = train_test_split(df_train_, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-10T17:03:09.308226Z",
     "start_time": "2022-02-10T17:03:09.076622Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train = df_train.drop(['label'], axis = 1)\n",
    "y_train = df_train[\"label\"].values\n",
    "\n",
    "x_valid = df_valid.drop(['label'], axis = 1)\n",
    "y_valid = df_valid[\"label\"].values\n",
    "\n",
    "x_test = df_test.drop(['label'], axis=1)\n",
    "y_test = df_test['label'].values\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "colnames_all = list(x_train.columns)\n",
    "x_train[colnames_all] = scaler.fit_transform(x_train[colnames_all])\n",
    "x_test = scaler.transform(x_test)\n",
    "x_valid[colnames_all] = scaler.fit_transform(x_valid[colnames_all])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF dataset만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-10T17:03:09.548720Z",
     "start_time": "2022-02-10T17:03:09.310515Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_tf_dataset(\n",
    "    X,\n",
    "    batch_size,\n",
    "    y = None,\n",
    "    shuffle = True, #false\n",
    "    drop_remainder = False,\n",
    "):\n",
    "    size_of_dataset = len(X)\n",
    "    if y is not None:\n",
    "        y = tf.one_hot(y.astype(int), 2)\n",
    "        ds = tf.data.Dataset.from_tensor_slices((np.array(X.astype(np.float32)), y))\n",
    "    else:\n",
    "        ds = tf.data.Dataset.from_tensor_slices(np.array(X.astype(np.float32)))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=size_of_dataset)\n",
    "    ds = ds.batch(batch_size, drop_remainder=drop_remainder)\n",
    "\n",
    "    autotune = tf.data.experimental.AUTOTUNE\n",
    "    ds = ds.prefetch(autotune)\n",
    "    return ds\n",
    "\n",
    "train_ds = prepare_tf_dataset(x_train, 1000, y_train)\n",
    "val_ds = prepare_tf_dataset(x_valid, 1000, y_valid)\n",
    "test_ds = prepare_tf_dataset(x_test, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-10T17:31:48.148042Z",
     "start_time": "2022-02-10T17:03:09.550899Z"
    },
    "hide_input": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method TabNet.call of <__main__.TabNet object at 0x7fbcff2716d8>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TabNet.call of <__main__.TabNet object at 0x7fbcff2716d8>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/py_3_6/lib/python3.6/site-packages/keras/layers/normalization/batch_normalization.py:520: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method FeatureTransformer.call of <__main__.FeatureTransformer object at 0x7fbd310f0a58>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method FeatureTransformer.call of <__main__.FeatureTransformer object at 0x7fbd310f0a58>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method FeatureBlock.call of <__main__.FeatureBlock object at 0x7fbcff271860>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method FeatureBlock.call of <__main__.FeatureBlock object at 0x7fbcff271860>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method AttentiveTransformer.call of <__main__.AttentiveTransformer object at 0x7fbd1144d1d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method AttentiveTransformer.call of <__main__.AttentiveTransformer object at 0x7fbd1144d1d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Train on 19 steps, validate on 5 steps\n",
      "Epoch 1/1000\n",
      "19/19 [==============================] - ETA: 0s - batch: 9.0000 - size: 1.0000 - loss: 0.6886 - output_1_loss: 0.6886"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/py_3_6/lib/python3.6/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 27s 1s/step - batch: 9.0000 - size: 1.0000 - loss: 0.6886 - output_1_loss: 0.6886 - val_loss: 0.6829 - val_output_1_loss: 0.6829\n",
      "Epoch 2/1000\n",
      "19/19 [==============================] - 26s 1s/step - batch: 9.0000 - size: 1.0000 - loss: 0.4213 - output_1_loss: 0.4213 - val_loss: 0.5989 - val_output_1_loss: 0.5989\n",
      "Epoch 3/1000\n",
      "19/19 [==============================] - 26s 1s/step - batch: 9.0000 - size: 1.0000 - loss: 0.2616 - output_1_loss: 0.2616 - val_loss: 0.4288 - val_output_1_loss: 0.4288\n",
      "Epoch 4/1000\n",
      "19/19 [==============================] - 27s 1s/step - batch: 9.0000 - size: 1.0000 - loss: 0.1649 - output_1_loss: 0.1649 - val_loss: 0.1011 - val_output_1_loss: 0.1011\n",
      "Epoch 5/1000\n",
      "19/19 [==============================] - 27s 1s/step - batch: 9.0000 - size: 1.0000 - loss: 0.1108 - output_1_loss: 0.1108 - val_loss: 0.0455 - val_output_1_loss: 0.0455\n",
      "Epoch 6/1000\n",
      "19/19 [==============================] - 26s 1s/step - batch: 9.0000 - size: 1.0000 - loss: 0.0798 - output_1_loss: 0.0798 - val_loss: 0.0648 - val_output_1_loss: 0.0648\n",
      "Epoch 7/1000\n",
      "19/19 [==============================] - 27s 1s/step - batch: 9.0000 - size: 1.0000 - loss: 0.0615 - output_1_loss: 0.0615 - val_loss: 0.0993 - val_output_1_loss: 0.0993\n",
      "Epoch 8/1000\n",
      "19/19 [==============================] - 26s 1s/step - batch: 9.0000 - size: 1.0000 - loss: 0.0488 - output_1_loss: 0.0488 - val_loss: 0.1039 - val_output_1_loss: 0.1039\n",
      "Epoch 9/1000\n",
      "19/19 [==============================] - 26s 1s/step - batch: 9.0000 - size: 1.0000 - loss: 0.0412 - output_1_loss: 0.0412 - val_loss: 0.1353 - val_output_1_loss: 0.1353\n",
      "Epoch 10/1000\n",
      "19/19 [==============================] - 26s 1s/step - batch: 9.0000 - size: 1.0000 - loss: 0.0354 - output_1_loss: 0.0354 - val_loss: 0.1271 - val_output_1_loss: 0.1271\n",
      "Epoch 11/1000\n",
      "19/19 [==============================] - 26s 1s/step - batch: 9.0000 - size: 1.0000 - loss: 0.0318 - output_1_loss: 0.0318 - val_loss: 0.0907 - val_output_1_loss: 0.0907\n",
      "Epoch 12/1000\n",
      "19/19 [==============================] - 26s 1s/step - batch: 9.0000 - size: 1.0000 - loss: 0.0283 - output_1_loss: 0.0283 - val_loss: 0.0833 - val_output_1_loss: 0.0833\n",
      "Epoch 13/1000\n",
      "19/19 [==============================] - 26s 1s/step - batch: 9.0000 - size: 1.0000 - loss: 0.0256 - output_1_loss: 0.0256 - val_loss: 0.0626 - val_output_1_loss: 0.0626\n",
      "Epoch 14/1000\n",
      "19/19 [==============================] - 27s 1s/step - batch: 9.0000 - size: 1.0000 - loss: 0.0240 - output_1_loss: 0.0240 - val_loss: 0.0503 - val_output_1_loss: 0.0503\n",
      "Epoch 15/1000\n",
      "19/19 [==============================] - 26s 1s/step - batch: 9.0000 - size: 1.0000 - loss: 0.0215 - output_1_loss: 0.0215 - val_loss: 0.0587 - val_output_1_loss: 0.0587\n",
      "Epoch 16/1000\n",
      "19/19 [==============================] - 26s 1s/step - batch: 9.0000 - size: 1.0000 - loss: 0.0198 - output_1_loss: 0.0198 - val_loss: 0.0406 - val_output_1_loss: 0.0406\n",
      "Epoch 17/1000\n",
      "19/19 [==============================] - 25s 1s/step - batch: 9.0000 - size: 1.0000 - loss: 0.0186 - output_1_loss: 0.0186 - val_loss: 0.0340 - val_output_1_loss: 0.0340\n",
      "Epoch 18/1000\n",
      "19/19 [==============================] - 27s 1s/step - batch: 9.0000 - size: 1.0000 - loss: 0.0178 - output_1_loss: 0.0178 - val_loss: 0.0391 - val_output_1_loss: 0.0391\n",
      "Epoch 19/1000\n",
      "19/19 [==============================] - 26s 1s/step - batch: 9.0000 - size: 1.0000 - loss: 0.0166 - output_1_loss: 0.0166 - val_loss: 0.0353 - val_output_1_loss: 0.0353\n",
      "Epoch 20/1000\n",
      "19/19 [==============================] - 26s 1s/step - batch: 9.0000 - size: 1.0000 - loss: 0.0158 - output_1_loss: 0.0158 - val_loss: 0.0302 - val_output_1_loss: 0.0302\n",
      "Epoch 21/1000\n",
      "19/19 [==============================] - 26s 1s/step - batch: 9.0000 - size: 1.0000 - loss: 0.0149 - output_1_loss: 0.0149 - val_loss: 0.0447 - val_output_1_loss: 0.0447\n",
      "Epoch 22/1000\n",
      "19/19 [==============================] - 28s 1s/step - batch: 9.0000 - size: 1.0000 - loss: 0.0139 - output_1_loss: 0.0139 - val_loss: 0.0282 - val_output_1_loss: 0.0282\n",
      "Epoch 23/1000\n",
      "19/19 [==============================] - 27s 1s/step - batch: 9.0000 - size: 1.0000 - loss: 0.0134 - output_1_loss: 0.0134 - val_loss: 0.0229 - val_output_1_loss: 0.0229\n",
      "Epoch 24/1000\n",
      "19/19 [==============================] - 26s 1s/step - batch: 9.0000 - size: 1.0000 - loss: 0.0125 - output_1_loss: 0.0125 - val_loss: 0.0234 - val_output_1_loss: 0.0234\n",
      "Epoch 25/1000\n",
      "19/19 [==============================] - 26s 1s/step - batch: 9.0000 - size: 1.0000 - loss: 0.0123 - output_1_loss: 0.0123 - val_loss: 0.0191 - val_output_1_loss: 0.0191\n",
      "Epoch 26/1000\n",
      "19/19 [==============================] - 27s 1s/step - batch: 9.0000 - size: 1.0000 - loss: 0.0120 - output_1_loss: 0.0120 - val_loss: 0.0187 - val_output_1_loss: 0.0187\n",
      "Epoch 27/1000\n",
      "19/19 [==============================] - 24s 1s/step - batch: 9.0000 - size: 1.0000 - loss: 0.0114 - output_1_loss: 0.0114 - val_loss: 0.0188 - val_output_1_loss: 0.0188\n",
      "Epoch 28/1000\n",
      "19/19 [==============================] - 26s 1s/step - batch: 9.0000 - size: 1.0000 - loss: 0.0113 - output_1_loss: 0.0113 - val_loss: 0.0186 - val_output_1_loss: 0.0186\n",
      "Epoch 29/1000\n",
      "19/19 [==============================] - 24s 1s/step - batch: 9.0000 - size: 1.0000 - loss: 0.0110 - output_1_loss: 0.0110 - val_loss: 0.0169 - val_output_1_loss: 0.0169\n",
      "Epoch 30/1000\n",
      "19/19 [==============================] - 25s 1s/step - batch: 9.0000 - size: 1.0000 - loss: 0.0110 - output_1_loss: 0.0110 - val_loss: 0.0181 - val_output_1_loss: 0.0181\n",
      "Epoch 31/1000\n",
      "19/19 [==============================] - 26s 1s/step - batch: 9.0000 - size: 1.0000 - loss: 0.0111 - output_1_loss: 0.0111 - val_loss: 0.0194 - val_output_1_loss: 0.0194\n",
      "Epoch 32/1000\n",
      "19/19 [==============================] - 26s 1s/step - batch: 9.0000 - size: 1.0000 - loss: 0.0110 - output_1_loss: 0.0110 - val_loss: 0.0215 - val_output_1_loss: 0.0215\n",
      "Epoch 33/1000\n",
      "19/19 [==============================] - 26s 1s/step - batch: 9.0000 - size: 1.0000 - loss: 0.0108 - output_1_loss: 0.0108 - val_loss: 0.0195 - val_output_1_loss: 0.0195\n",
      "Epoch 34/1000\n",
      "19/19 [==============================] - 26s 1s/step - batch: 9.0000 - size: 1.0000 - loss: 0.0112 - output_1_loss: 0.0112 - val_loss: 0.0193 - val_output_1_loss: 0.0193\n",
      "Epoch 35/1000\n",
      "19/19 [==============================] - 26s 1s/step - batch: 9.0000 - size: 1.0000 - loss: 0.0110 - output_1_loss: 0.0110 - val_loss: 0.0187 - val_output_1_loss: 0.0187\n",
      "Epoch 36/1000\n",
      "19/19 [==============================] - 27s 1s/step - batch: 9.0000 - size: 1.0000 - loss: 0.0107 - output_1_loss: 0.0107 - val_loss: 0.0167 - val_output_1_loss: 0.0167\n",
      "Epoch 37/1000\n",
      "19/19 [==============================] - 26s 1s/step - batch: 9.0000 - size: 1.0000 - loss: 0.0105 - output_1_loss: 0.0105 - val_loss: 0.0203 - val_output_1_loss: 0.0203\n",
      "Epoch 38/1000\n",
      "19/19 [==============================] - 28s 1s/step - batch: 9.0000 - size: 1.0000 - loss: 0.0103 - output_1_loss: 0.0103 - val_loss: 0.0219 - val_output_1_loss: 0.0219\n",
      "Epoch 39/1000\n",
      "19/19 [==============================] - 26s 1s/step - batch: 9.0000 - size: 1.0000 - loss: 0.0104 - output_1_loss: 0.0104 - val_loss: 0.0206 - val_output_1_loss: 0.0206\n",
      "Epoch 40/1000\n",
      "19/19 [==============================] - 26s 1s/step - batch: 9.0000 - size: 1.0000 - loss: 0.0098 - output_1_loss: 0.0098 - val_loss: 0.0209 - val_output_1_loss: 0.0209\n",
      "Epoch 41/1000\n",
      "19/19 [==============================] - 25s 1s/step - batch: 9.0000 - size: 1.0000 - loss: 0.0095 - output_1_loss: 0.0095 - val_loss: 0.0222 - val_output_1_loss: 0.0222\n",
      "Epoch 42/1000\n",
      "19/19 [==============================] - 25s 1s/step - batch: 9.0000 - size: 1.0000 - loss: 0.0095 - output_1_loss: 0.0095 - val_loss: 0.0198 - val_output_1_loss: 0.0198\n",
      "Epoch 43/1000\n",
      "19/19 [==============================] - 25s 1s/step - batch: 9.0000 - size: 1.0000 - loss: 0.0088 - output_1_loss: 0.0088 - val_loss: 0.0188 - val_output_1_loss: 0.0188\n",
      "Epoch 44/1000\n",
      "19/19 [==============================] - 25s 1s/step - batch: 9.0000 - size: 1.0000 - loss: 0.0086 - output_1_loss: 0.0086 - val_loss: 0.0215 - val_output_1_loss: 0.0215\n",
      "Epoch 45/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 25s 1s/step - batch: 9.0000 - size: 1.0000 - loss: 0.0082 - output_1_loss: 0.0082 - val_loss: 0.0239 - val_output_1_loss: 0.0239\n",
      "Epoch 46/1000\n",
      "19/19 [==============================] - 26s 1s/step - batch: 9.0000 - size: 1.0000 - loss: 0.0081 - output_1_loss: 0.0081 - val_loss: 0.0212 - val_output_1_loss: 0.0212\n",
      "Epoch 47/1000\n",
      "19/19 [==============================] - 25s 1s/step - batch: 9.0000 - size: 1.0000 - loss: 0.0079 - output_1_loss: 0.0079 - val_loss: 0.0207 - val_output_1_loss: 0.0207\n",
      "Epoch 48/1000\n",
      "19/19 [==============================] - 24s 1s/step - batch: 9.0000 - size: 1.0000 - loss: 0.0076 - output_1_loss: 0.0076 - val_loss: 0.0205 - val_output_1_loss: 0.0205\n",
      "Epoch 49/1000\n",
      "19/19 [==============================] - 25s 1s/step - batch: 9.0000 - size: 1.0000 - loss: 0.0075 - output_1_loss: 0.0075 - val_loss: 0.0192 - val_output_1_loss: 0.0192\n",
      "Epoch 50/1000\n",
      "19/19 [==============================] - 26s 1s/step - batch: 9.0000 - size: 1.0000 - loss: 0.0077 - output_1_loss: 0.0077 - val_loss: 0.0212 - val_output_1_loss: 0.0212\n",
      "Epoch 51/1000\n",
      "19/19 [==============================] - 25s 1s/step - batch: 9.0000 - size: 1.0000 - loss: 0.0076 - output_1_loss: 0.0076 - val_loss: 0.0191 - val_output_1_loss: 0.0191\n",
      "Epoch 52/1000\n",
      "19/19 [==============================] - 25s 1s/step - batch: 9.0000 - size: 1.0000 - loss: 0.0072 - output_1_loss: 0.0072 - val_loss: 0.0209 - val_output_1_loss: 0.0209\n",
      "Epoch 53/1000\n",
      "19/19 [==============================] - 25s 1s/step - batch: 9.0000 - size: 1.0000 - loss: 0.0065 - output_1_loss: 0.0065 - val_loss: 0.0219 - val_output_1_loss: 0.0219\n",
      "Epoch 54/1000\n",
      "19/19 [==============================] - 27s 1s/step - batch: 9.0000 - size: 1.0000 - loss: 0.0060 - output_1_loss: 0.0060 - val_loss: 0.0201 - val_output_1_loss: 0.0201\n",
      "Epoch 55/1000\n",
      "19/19 [==============================] - 26s 1s/step - batch: 9.0000 - size: 1.0000 - loss: 0.0061 - output_1_loss: 0.0061 - val_loss: 0.0213 - val_output_1_loss: 0.0213\n",
      "Epoch 56/1000\n",
      "19/19 [==============================] - 26s 1s/step - batch: 9.0000 - size: 1.0000 - loss: 0.0065 - output_1_loss: 0.0065 - val_loss: 0.0182 - val_output_1_loss: 0.0182\n",
      "Epoch 57/1000\n",
      "19/19 [==============================] - 26s 1s/step - batch: 9.0000 - size: 1.0000 - loss: 0.0063 - output_1_loss: 0.0063 - val_loss: 0.0489 - val_output_1_loss: 0.0489\n",
      "Epoch 58/1000\n",
      "19/19 [==============================] - 24s 1s/step - batch: 9.0000 - size: 1.0000 - loss: 0.0063 - output_1_loss: 0.0063 - val_loss: 0.0276 - val_output_1_loss: 0.0276\n",
      "Epoch 59/1000\n",
      "19/19 [==============================] - 27s 1s/step - batch: 9.0000 - size: 1.0000 - loss: 0.0060 - output_1_loss: 0.0060 - val_loss: 0.0280 - val_output_1_loss: 0.0280\n",
      "Epoch 60/1000\n",
      "19/19 [==============================] - 26s 1s/step - batch: 9.0000 - size: 1.0000 - loss: 0.0062 - output_1_loss: 0.0062 - val_loss: 0.0324 - val_output_1_loss: 0.0324\n",
      "Epoch 61/1000\n",
      "19/19 [==============================] - 26s 1s/step - batch: 9.0000 - size: 1.0000 - loss: 0.0059 - output_1_loss: 0.0059 - val_loss: 0.0239 - val_output_1_loss: 0.0239\n",
      "Epoch 62/1000\n",
      "19/19 [==============================] - 26s 1s/step - batch: 9.0000 - size: 1.0000 - loss: 0.0055 - output_1_loss: 0.0055 - val_loss: 0.0273 - val_output_1_loss: 0.0273\n",
      "Epoch 63/1000\n",
      "19/19 [==============================] - 26s 1s/step - batch: 9.0000 - size: 1.0000 - loss: 0.0055 - output_1_loss: 0.0055 - val_loss: 0.0336 - val_output_1_loss: 0.0336\n",
      "Epoch 64/1000\n",
      "19/19 [==============================] - 26s 1s/step - batch: 9.0000 - size: 1.0000 - loss: 0.0056 - output_1_loss: 0.0056 - val_loss: 0.0360 - val_output_1_loss: 0.0360\n",
      "Epoch 65/1000\n",
      "19/19 [==============================] - 26s 1s/step - batch: 9.0000 - size: 1.0000 - loss: 0.0054 - output_1_loss: 0.0054 - val_loss: 0.0586 - val_output_1_loss: 0.0586\n",
      "Epoch 66/1000\n",
      "19/19 [==============================] - 27s 1s/step - batch: 9.0000 - size: 1.0000 - loss: 0.0054 - output_1_loss: 0.0054 - val_loss: 0.0285 - val_output_1_loss: 0.0285\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbd1347af60>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tabnet = TabNet(num_features = x_train.shape[1],\n",
    "                output_dim = 128,\n",
    "                feature_dim = 128,\n",
    "                n_step = 2, \n",
    "                relaxation_factor= 2.2,\n",
    "                sparsity_coefficient=2.37e-07,\n",
    "                n_shared = 2,\n",
    "                bn_momentum = 0.9245)\n",
    "\n",
    "\n",
    "# Early stopping based on validation loss    \n",
    "cbs = [tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\", patience=30, restore_best_weights=True\n",
    "    )]\n",
    "\n",
    "# Optimiser \n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001, clipnorm=10)\n",
    "\n",
    "# Second loss in None because we also output the importances\n",
    "loss = [tf.keras.losses.CategoricalCrossentropy(from_logits=False), None]\n",
    "\n",
    "# Compile the model\n",
    "tabnet.compile(optimizer,\n",
    "               loss=loss)\n",
    "\n",
    "# Train the model\n",
    "tabnet.fit(train_ds, \n",
    "           epochs=1000, \n",
    "           validation_data=val_ds,\n",
    "           callbacks=cbs,\n",
    "           verbose=1,\n",
    "          class_weight={\n",
    "              0:1,\n",
    "              1: 10\n",
    "          })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation 성능 확인 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-10T17:31:51.786681Z",
     "start_time": "2022-02-10T17:31:48.149775Z"
    }
   },
   "outputs": [],
   "source": [
    "val_preds, val_imps = tabnet.predict(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-10T17:32:52.318446Z",
     "start_time": "2022-02-10T17:32:52.287005Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ROC AUC 0.4855\n",
      "Test PR AUC 0.002\n"
     ]
    }
   ],
   "source": [
    "print('Test ROC AUC', np.round(roc_auc_score(y_valid, val_preds[:, 1]), 4))\n",
    "print('Test PR AUC', np.round(average_precision_score(y_valid, val_preds[:, 1]), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-10T17:32:58.206205Z",
     "start_time": "2022-02-10T17:32:54.610940Z"
    }
   },
   "outputs": [],
   "source": [
    "test_preds, test_imp = tabnet.predict(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-10T17:32:58.643592Z",
     "start_time": "2022-02-10T17:32:58.640509Z"
    }
   },
   "outputs": [],
   "source": [
    "temp = pd.DataFrame({'y':y_test, 'yhat':test_preds[:,1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-10T17:35:03.399232Z",
     "start_time": "2022-02-10T17:35:03.395037Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 ... 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.where(test_preds[:, 1] > 0.005, 1, 0)\n",
    "# y_pred = np.percentile(test_preds[:, 1],5)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-10T17:35:04.487813Z",
     "start_time": "2022-02-10T17:35:04.405059Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score : 0.63\n",
      "Precision : 0.0\n",
      "Recall : 0.38\n",
      "F1 : 0.0\n"
     ]
    }
   ],
   "source": [
    "print ('Accuracy Score :',np.round(accuracy_score(y_test, y_pred),2) )\n",
    "print ('Precision :',np.round(precision_score(y_test, y_pred),2))\n",
    "print ('Recall :',np.round(recall_score(y_test, y_pred),2))\n",
    "print ('F1 :',np.round(f1_score(y_test, y_pred),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-10T17:35:05.567820Z",
     "start_time": "2022-02-10T17:35:05.362339Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix :\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "target_names = ['non_fraud', 'fraud']\n",
    "\n",
    "print ('Confusion Matrix :')\n",
    "\n",
    "def plot_confusion_matrix(cm,target_names, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.figure(figsize=(5,5),)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(target_names))\n",
    "    plt.xticks(tick_marks, target_names, rotation=45)\n",
    "    plt.yticks(tick_marks, target_names)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    width, height = cm.shape\n",
    "\n",
    "    for x in range(width):\n",
    "        for y in range(height):\n",
    "            plt.annotate(str(cm[x][y]), xy=(y, x), \n",
    "                        horizontalalignment='center',\n",
    "                        verticalalignment='center')\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "plot_confusion_matrix(cm,target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 의문점\n",
    "   - self-supervised 부분이 autoencoder랑 다른점이 무엇인지?\n",
    "   - !!!이미 package가 나와있음!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 참고 문헌 / 코드\n",
    "\n",
    "* https://today-1.tistory.com/54\n",
    "* https://dacon.io/codeshare/2515\n",
    "* https://velog.io/@jkl133/TabNet-Gradient-Boosting%EC%9D%98-%EC%A2%85%EB%A7%90\n",
    "* https://data-newbie.tistory.com/378?category=753185\n",
    "</br>\n",
    "\n",
    "* https://github.com/google-research/google-research\n",
    "* https://github.com/aruberts/blogs/blob/main/TabNet%20Classification%20Example.ipynb\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_3_6",
   "language": "python",
   "name": "py_3_6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
